{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronascimento/pyLiBELa/blob/main/Colabs/TF/(GVM)_Preparing_PDBBind_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha0zM_nV0hbU",
        "outputId": "eb6ab966-73a9-45f2-a924-cb62d0647f6b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pdbbind  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = 'smart-monitor-401017'\n",
        "!gcloud config set project {project_id}\n",
        "!gsutil ls"
      ],
      "metadata": {
        "id": "bCVeuViUlXvM",
        "outputId": "06734f24-59fd-4f74-dcd3-8482995205ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "gs://pdbbind/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper class to handle GCS file access\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "class AccessFile():\n",
        "  \"\"\"\n",
        "  Upon initialization, downloads the file stored in the cloud storage\n",
        "  as given by the cgs_path rooted in gs://pdbbind/ and stores it in\n",
        "  Colab's /content/tmp directory. The file is deleted once all the\n",
        "  references to the object are dropped.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, gcs_path, log=False):\n",
        "    self.gcs_path = gcs_path\n",
        "    self.local_path = f'/content/tmp/{self.gcs_path}'\n",
        "\n",
        "    self.copy_status = !gsutil cp gs://pdbbind/{gcs_path} {self.local_path}\n",
        "    if log: print(self.copy_status)\n",
        "\n",
        "    self.is_open = False\n",
        "    if not Path(self.local_path).exists():\n",
        "      raise FileNotFoundError(f'File {self.local_path} not found. \\\n",
        "      \\nThis Google Cloud Storage copy operation returned {self.copy_status}')\n",
        "\n",
        "  def open(self, mode):\n",
        "    self.is_open = True\n",
        "    self._file_handle = open(self.local_path, mode)\n",
        "    return self\n",
        "\n",
        "  def getFile(self):\n",
        "    return self._file_handle\n",
        "\n",
        "  def getPath(self):\n",
        "    return self.local_path\n",
        "\n",
        "  def __del__(self):\n",
        "    if self.is_open: self.getFile().close()\n",
        "    deletion = !rm -f {self.local_path}"
      ],
      "metadata": {
        "id": "f-w6VVHNokzB",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Getting list of targets with grids\n",
        "targets_ok = []\n",
        "target_ok_file = AccessFile('scripts/targets_ok.dat').open('r')\n",
        "for line in target_ok_file.getFile():\n",
        "  line2 = line.split()\n",
        "  targets_ok.append(line2[0])\n",
        "target_ok_file.getFile().close()"
      ],
      "metadata": {
        "id": "Tut1ZZ_T48ed",
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reading PDBBind v2020 data\n",
        "binding_targets = []             # pdb id\n",
        "binding_years = []               # year of the structure\n",
        "binding_resolution = []          # resolution\n",
        "binding_score = []               # -logKd/Ki\n",
        "\n",
        "binding_file = AccessFile('index/INDEX_general_PL_data.2020').open('r')\n",
        "for line in binding_file.getFile():\n",
        "  line2 = line.split()\n",
        "  if line2[0][0] != '#':\n",
        "    if line2[0] not in targets_ok: continue\n",
        "\n",
        "    binding_targets.append(line2[0])\n",
        "    if (line2[1] == 'NMR'):\n",
        "      binding_resolution.append(0.00)\n",
        "    else:\n",
        "      binding_resolution.append(float(line2[1]))\n",
        "    binding_years.append(int(line2[2]))\n",
        "    binding_score.append(float(line2[3]))\n",
        "binding_file.getFile().close()\n",
        "\n",
        "print('Binding data found for %d valid targets' % len(binding_targets))"
      ],
      "metadata": {
        "id": "R_XETC5FIITe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab7bf95-65c1-4938-f51b-9d934a7d27a0",
        "cellView": "form"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binding data found for 18925 valid targets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Auxiliary functions\n",
        "import tensorflow as tf\n",
        "\n",
        "def process_file(file_access : AccessFile):\n",
        "\n",
        "  data = tf.io.read_file(file_access.getPath())\n",
        "  data = tf.io.decode_raw(data, tf.float64)\n",
        "  data = tf.reshape(data, (60, 60, 60, 3))\n",
        "  return data\n",
        "\n",
        "def _log_failure(message):\n",
        "  with open(\"log_files_not_found\", 'a') as f:\n",
        "    f.write(f'{message}\\n')\n",
        "\n",
        "def data_generator(name_list, score_list):\n",
        "\n",
        "  for i in range(len(name_list)):\n",
        "\n",
        "    target = name_list[i].decode()\n",
        "    root_path = f'targets/{target}/grid_30_0.5_SF0'\n",
        "\n",
        "    try:\n",
        "      grid1_access = AccessFile(f'{root_path}/McGrid_rec.grid')\n",
        "      grid2_access = AccessFile(f'{root_path}/McGrid_lig.grid')\n",
        "    except FileNotFoundError as err:\n",
        "      _log_failure(f'rec or lig \"{target}\" not found with error <<{err}>>\\n')\n",
        "      continue\n",
        "\n",
        "    grid1 = process_file(grid1_access)\n",
        "    grid2 = process_file(grid2_access)\n",
        "\n",
        "    observable = score_list[i]\n",
        "    combined_grid = tf.concat([grid1, grid2], axis=-1)\n",
        "\n",
        "    # Yield the data as a tuple\n",
        "    yield combined_grid, observable\n",
        "\n",
        "    # Yields the 10 related decoys of the current grid\n",
        "    for j in range(1,11):\n",
        "\n",
        "      try:\n",
        "        grid3_access = AccessFile(f'{root_path}/McGrid_dec_{j}.grid')\n",
        "      except FileNotFoundError as err:\n",
        "        _log_failure(f'Decoy \"{target}\" not found with error {err}')\n",
        "        continue\n",
        "\n",
        "      grid3 = process_file(grid3_access)\n",
        "      observable = 0.00\n",
        "      combined_grid = tf.concat([grid1, grid3], axis=-1)\n",
        "      yield combined_grid, observable\n"
      ],
      "metadata": {
        "id": "UOoG_Q3u5FH6",
        "cellView": "form"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reading dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_names, test_names, train_scores, test_scores = train_test_split(binding_targets, binding_score, train_size=0.8, shuffle=True)\n",
        "train_names, valid_names, train_scores, valid_scores = train_test_split(train_names, train_scores, train_size=0.8)\n",
        "\n",
        "def slice_data(dataset):\n",
        "  x = tf.concat([x for x, y in dataset], axis=-1)\n",
        "  y = tf.concat([y for x, y in dataset], axis=-1)\n",
        "  return x,y\n",
        "\n",
        "# Some dataset parameters\n",
        "\n",
        "output_signature = (tf.TensorSpec(shape=(60, 60, 60, 6), dtype=tf.float64, name='combgrid'),\n",
        "                    tf.TensorSpec(shape=(), dtype=tf.float32, name='ligscore'))\n",
        "batch_size = 5\n",
        "prefetch_size = 1\n",
        "\n",
        "# Create the dataset from the generator function,\n",
        "# with batch and prefetch sizes already determined\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator,\n",
        "    output_signature=output_signature,\n",
        "    args=(tf.convert_to_tensor(train_names, dtype=tf.string), tf.convert_to_tensor(train_scores, dtype=tf.float32)),\n",
        "    name=\"train_dataset_gen\"\n",
        ").batch(batch_size).prefetch(prefetch_size)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator,\n",
        "    output_signature=output_signature,\n",
        "    args=(tf.convert_to_tensor(test_names, dtype=tf.string), tf.convert_to_tensor(test_scores, dtype=tf.float32)),\n",
        "    name=\"test_dataset_gen\"\n",
        ").batch(batch_size).prefetch(prefetch_size)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator,\n",
        "    output_signature=output_signature,\n",
        "    args=(tf.convert_to_tensor(valid_names, dtype=tf.string), tf.convert_to_tensor(valid_scores, dtype=tf.float32)),\n",
        "    name=\"valid_dataset_gen\"\n",
        ").batch(batch_size).prefetch(prefetch_size)\n",
        "\n",
        "# train_val, test = tf.keras.utils.split_dataset(dataset, left_size=0.8, right_size=0.2, shuffle=True, seed=23)\n",
        "# train, valid = tf.keras.utils.split_dataset(train_val, left_size=0.8, right_size=0.2)\n",
        "\n",
        "# x_train, y_train = slice_data(train)\n",
        "# x_test, y_test = slice_data(test)\n",
        "# x_valid, y_valid = slice_data(valid)\n",
        "\n",
        "# Repeat the dataset for multiple epochs (optional)\n",
        "#dataset = dataset.repeat()"
      ],
      "metadata": {
        "id": "_TE_iVYIDUAW",
        "cellView": "form"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Alex's model\n",
        "\n",
        "cnn_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv3D(64, 5, activation='relu', data_format='channels_last', input_shape=(60,60,60,6), padding=\"same\"),\n",
        "        tf.keras.layers.MaxPool3D(data_format='channels_last'),\n",
        "        tf.keras.layers.Conv3D(128, 3, activation='relu', data_format='channels_last', padding=\"same\"),\n",
        "        tf.keras.layers.Conv3D(128, 3, activation='relu', data_format='channels_last', padding=\"same\"),\n",
        "        tf.keras.layers.MaxPool3D(data_format='channels_last'),\n",
        "        tf.keras.layers.Conv3D(256, 3, activation='relu', data_format='channels_last', padding=\"same\"),\n",
        "        tf.keras.layers.Conv3D(256, 3, activation='relu', data_format='channels_last', padding=\"same\"),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(units=128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(units=64, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(units=1),\n",
        "    ])\n",
        "\n",
        "cnn_model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=[\"RootMeanSquaredError\"])"
      ],
      "metadata": {
        "id": "RLkC3ZiaLkRm",
        "cellView": "form"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_info = cnn_model.fit(train_dataset, epochs=1, validation_data=valid_dataset)\n",
        "mse_test = cnn_model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "czS-cMheL4TL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcd817eb-be1f-4b68-b778-c66660b17f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://pdbbind/targets/3dbs/grid_30_0.5_SF0/McGrid_dec_9.grid...\n",
            "/ [0 files][    0.0 B/  4.9 MiB]                                                \r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Questions:\n",
        "\n",
        "1. What if we used a batch normalization in the model? Something like:\n",
        "\n",
        "```\n",
        "tf.keras.layers.BatchNormalization()\n",
        "```\n",
        "\n",
        "in the begining of the model and after each hidden layer?\n"
      ],
      "metadata": {
        "id": "psJtWtTITA6F"
      }
    }
  ]
}