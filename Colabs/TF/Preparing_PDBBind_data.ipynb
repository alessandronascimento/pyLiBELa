{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronascimento/pyLiBELa/blob/main/Colabs/TF/Preparing_PDBBind_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Conecting to Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/Projects_Data/PDBbind_v2020/scripts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD_wZorcHhrB",
        "outputId": "35afef3c-2689-4090-df83-34a31b039a1a",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/My Drive/Projects_Data/PDBbind_v2020/scripts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reading PDBBind v2020 data\n",
        "binding_targets = []             # pdb id\n",
        "binding_years = []               # year of the structure\n",
        "binding_resolution = []          # resolution\n",
        "binding_score = []               # -logKd/Ki\n",
        "\n",
        "binding_file = open('../index/INDEX_general_PL_data.2020', 'r')\n",
        "for line in binding_file:\n",
        "  line2 = line.split()\n",
        "  if line2[0][0] != '#':\n",
        "    binding_targets.append(line2[0])\n",
        "    if (line2[1] == 'NMR'):\n",
        "      binding_resolution.append(0.00)\n",
        "    else:\n",
        "      binding_resolution.append(float(line2[1]))\n",
        "    binding_years.append(int(line2[2]))\n",
        "    binding_score.append(float(line2[3]))\n",
        "binding_file.close()\n",
        "\n",
        "print('Binding data found for %d targets' % len(binding_targets))"
      ],
      "metadata": {
        "id": "R_XETC5FIITe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d23d0389-dad3-4ceb-9b65-dc17c5298874",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binding data found for 19443 targets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Getting list of targets with grids\n",
        "targets_ok = []\n",
        "target_ok_file = open('targets_ok.dat', 'r')\n",
        "for line in target_ok_file:\n",
        "  line2 = line.split()\n",
        "  targets_ok.append(line2[0])\n",
        "target_ok_file.close()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Tut1ZZ_T48ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Auxiliary functions\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def process_file(file_path):\n",
        "  data = tf.io.read_file(file_path)\n",
        "  data = tf.io.decode_raw(data, tf.float64)\n",
        "  data = tf.reshape(data, (-1, 60, 60, 3))\n",
        "  return data\n",
        "\n",
        "\n",
        "def data_generator():\n",
        "  ntargets=20\n",
        "  for i in range(ntargets): #len(targets_ok)):\n",
        "    target = targets_ok[i]\n",
        "    file_path = '../targets/{}/grid_30_0.5_SF0'.format(target)\n",
        "    grid1 = process_file(file_path + '/McGrid_rec.grid')\n",
        "    grid2 = process_file(file_path + '/McGrid_lig.grid')\n",
        "\n",
        "    idx = binding_targets.index(target)\n",
        "    observable = binding_score[idx]\n",
        "    combined_grid = tf.concat([grid1, grid2], axis=-1)\n",
        "\n",
        "    # Yield the data as a tuple\n",
        "    yield combined_grid, observable\n",
        "#    yield grid1, grid2, observable\n",
        "\n",
        "  for i in range(ntargets): #len(targets_ok)):\n",
        "    target = targets_ok[i]\n",
        "    file_path = '../targets/{}/grid_30_0.5_SF0'.format(target)\n",
        "    grid1 = process_file(file_path + '/McGrid_rec.grid')\n",
        "\n",
        "    for j in range(1,11):\n",
        "      grid2 = process_file(file_path + '/McGrid_dec_{}.grid'.format(j))\n",
        "      observable = 0.00\n",
        "      combined_grid = tf.concat([grid1, grid2], axis=-1)\n",
        "#      yield grid1, grid2, observable\n",
        "      yield combined_grid, observable\n"
      ],
      "metadata": {
        "id": "UOoG_Q3u5FH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reading dataset\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def slice_data(dataset):\n",
        "  x = tf.concat([x for x, y in dataset], axis=-1)\n",
        "  y = tf.concat([y for x, y in dataset], axis=-1)\n",
        "  return x,y\n",
        "\n",
        "\n",
        "\n",
        "# Create the dataset from the generator function\n",
        "dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator,\n",
        "    output_signature=(tf.TensorSpec(shape=(None, 60, 60, 6), dtype=tf.float64, name='combgrid'), tf.TensorSpec(shape=(), dtype=tf.float32, name='ligscore')),\n",
        ")\n",
        "\n",
        "dataset = dataset.batch(batch_size=10, drop_remainder=True)\n",
        "\n",
        "train_val, test = tf.keras.utils.split_dataset(dataset, left_size=0.8, right_size=0.2, shuffle=True, seed=23)\n",
        "train, valid = tf.keras.utils.split_dataset(train_val, left_size=0.8, right_size=0.2)\n",
        "\n",
        "x_train, y_train = slice_data(train)\n",
        "x_test, y_test = slice_data(test)\n",
        "x_valid, y_valid = slice_data(valid)\n",
        "\n",
        "# Repeat the dataset for multiple epochs (optional)\n",
        "#dataset = dataset.repeat()"
      ],
      "metadata": {
        "id": "_TE_iVYIDUAW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "f1a19359-bc1d-4046-acc5-be1f8bfe96fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-bf3b8df72348>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtrain_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36msplit_dataset\u001b[0;34m(dataset, left_size, right_size, shuffle, seed)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_type_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     )\n\u001b[0;32m--> 100\u001b[0;31m     right_split = _restore_dataset_from_list(\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mright_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_type_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36m_restore_dataset_from_list\u001b[0;34m(dataset_as_list, dataset_type_spec, original_dataset)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrestored_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdataset_as_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset_as_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrestored_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdataset_as_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset_as_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Alex's model\n",
        "\n",
        "cnn_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv3D(64, 5, activation='relu', data_format='channels_first', input_shape=(60,60,60,6), padding=\"same\"),\n",
        "        tf.keras.layers.MaxPool3D(data_format='channels_first'),\n",
        "        tf.keras.layers.Conv3D(128, 3, activation='relu', data_format='channels_first', padding=\"same\"),\n",
        "        tf.keras.layers.Conv3D(128, 3, activation='relu', data_format='channels_first', padding=\"same\"),\n",
        "        tf.keras.layers.MaxPool3D(data_format='channels_first'),\n",
        "        tf.keras.layers.Conv3D(256, 3, activation='relu', data_format='channels_first', padding=\"same\"),\n",
        "        tf.keras.layers.Conv3D(256, 3, activation='relu', data_format='channels_first', padding=\"same\"),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(units=128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(units=64, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(units=1),\n",
        "    ])\n",
        "\n",
        "cnn_model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=[\"RootMeanSquaredError\"])"
      ],
      "metadata": {
        "id": "RLkC3ZiaLkRm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_info = cnn_model.fit(x_train, y_train, epochs=20, validation_data=(x_valid, y_valid))\n",
        "mse_test = cnn_model.evaluate(x_test, y_test)\n",
        "#mse_test, rmse_test = cnn_model.evaluate(test.batch(BATCH))"
      ],
      "metadata": {
        "id": "czS-cMheL4TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Questions:\n",
        "\n",
        "1. What if we used a batch normalization in the model? Something like:\n",
        "\n",
        "```\n",
        "tf.keras.layers.BatchNormalization()\n",
        "```\n",
        "\n",
        "in the begining of the model and after each hidden layer?\n"
      ],
      "metadata": {
        "id": "psJtWtTITA6F"
      }
    }
  ]
}