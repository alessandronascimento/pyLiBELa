{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronascimento/pyLiBELa/blob/main/Colabs/TF/Copy_of_Preparing_PDBBind_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Conecting to Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/PDBbind_v2020/scripts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD_wZorcHhrB",
        "outputId": "53a58861-e038-4b34-b361-0e7562f05d4c",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/.shortcut-targets-by-id/1yaTdYVpjmL1DAJ6BUmgTH5HVaSbYH99b/PDBbind_v2020/scripts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Getting list of targets with grids\n",
        "targets_ok = []\n",
        "target_ok_file = open('targets_ok.dat', 'r')\n",
        "for line in target_ok_file:\n",
        "  line2 = line.split()\n",
        "  targets_ok.append(line2[0])\n",
        "target_ok_file.close()"
      ],
      "metadata": {
        "id": "Tut1ZZ_T48ed"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reading PDBBind v2020 data\n",
        "binding_targets = []             # pdb id\n",
        "binding_years = []               # year of the structure\n",
        "binding_resolution = []          # resolution\n",
        "binding_score = []               # -logKd/Ki\n",
        "\n",
        "binding_file = open('../index/INDEX_general_PL_data.2020', 'r')\n",
        "for line in binding_file:\n",
        "  line2 = line.split()\n",
        "  if line2[0][0] != '#':\n",
        "    if line2[0] not in targets_ok: continue\n",
        "\n",
        "    binding_targets.append(line2[0])\n",
        "    if (line2[1] == 'NMR'):\n",
        "      binding_resolution.append(0.00)\n",
        "    else:\n",
        "      binding_resolution.append(float(line2[1]))\n",
        "    binding_years.append(int(line2[2]))\n",
        "    binding_score.append(float(line2[3]))\n",
        "binding_file.close()\n",
        "\n",
        "print('Binding data found for %d targets' % len(binding_targets))"
      ],
      "metadata": {
        "id": "R_XETC5FIITe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f2fa9b-1e02-42c2-cfa4-f2bb6da93258"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binding data found for 18925 targets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Auxiliary functions\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def process_file(file_path):\n",
        "  data = tf.io.read_file(file_path)\n",
        "  data = tf.io.decode_raw(data, tf.float64)\n",
        "  data = tf.reshape(data, (60, 60, 60, 3))\n",
        "  return data\n",
        "\n",
        "\n",
        "def data_generator(name_list, score_list):\n",
        "  ntargets=20\n",
        "  for i in range(ntargets): #len(targets_ok)):\n",
        "    #target = targets_ok[i]\n",
        "    target = name_list[i]\n",
        "    file_path = '../targets/{}/grid_30_0.5_SF0'.format(target)\n",
        "    grid1 = process_file(file_path + '/McGrid_rec.grid')\n",
        "    grid2 = process_file(file_path + '/McGrid_lig.grid')\n",
        "\n",
        "    #idx = binding_targets.index(target)\n",
        "    observable = score_list[i]\n",
        "    combined_grid = tf.concat([grid1, grid2], axis=-1)\n",
        "\n",
        "    # Yield the data as a tuple\n",
        "    yield combined_grid, observable\n",
        "#    yield grid1, grid2, observable\n",
        "\n",
        "  for i in range(ntargets): #len(targets_ok)):\n",
        "    #target = targets_ok[i]\n",
        "    target = name_list[i]\n",
        "    file_path = '../targets/{}/grid_30_0.5_SF0'.format(target)\n",
        "    grid1 = process_file(file_path + '/McGrid_rec.grid')\n",
        "\n",
        "    for j in range(1,11):\n",
        "      grid2 = process_file(file_path + '/McGrid_dec_{}.grid'.format(j))\n",
        "      observable = 0.00\n",
        "      combined_grid = tf.concat([grid1, grid2], axis=-1)\n",
        "#      yield grid1, grid2, observable\n",
        "      yield combined_grid, observable\n"
      ],
      "metadata": {
        "id": "UOoG_Q3u5FH6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reading dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_names, test_names, train_scores, test_scores = train_test_split(binding_targets, binding_score, train_size=0.8, shuffle=True)\n",
        "train_names, valid_names, train_scores, valid_scores = train_test_split(train_names, train_scores, train_size=0.8)\n",
        "\n",
        "\n",
        "def slice_data(dataset):\n",
        "  x = tf.concat([x for x, y in dataset], axis=-1)\n",
        "  y = tf.concat([y for x, y in dataset], axis=-1)\n",
        "  return x,y\n",
        "\n",
        "output_signature = (tf.TensorSpec(shape=(60, 60, 60, 6), dtype=tf.float64, name='combgrid'),\n",
        "                    tf.TensorSpec(shape=(), dtype=tf.float32, name='ligscore'))\n",
        "\n",
        "# Create the dataset from the generator function\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator,\n",
        "    output_signature=output_signature,\n",
        "    args = (tf.convert_to_tensor(train_names), tf.convert_to_tensor(train_scores))\n",
        ")\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator,\n",
        "    output_signature=output_signature,\n",
        "    args = (tf.convert_to_tensor(test_names), tf.convert_to_tensor(test_scores))\n",
        ")\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator,\n",
        "    output_signature=output_signature,\n",
        "    args = (tf.convert_to_tensor(valid_names), tf.convert_to_tensor(valid_scores))\n",
        ")\n",
        "\n",
        "# train_val, test = tf.keras.utils.split_dataset(dataset, left_size=0.8, right_size=0.2, shuffle=True, seed=23)\n",
        "# train, valid = tf.keras.utils.split_dataset(train_val, left_size=0.8, right_size=0.2)\n",
        "\n",
        "# x_train, y_train = slice_data(train)\n",
        "# x_test, y_test = slice_data(test)\n",
        "# x_valid, y_valid = slice_data(valid)\n",
        "\n",
        "# Repeat the dataset for multiple epochs (optional)\n",
        "#dataset = dataset.repeat()"
      ],
      "metadata": {
        "id": "_TE_iVYIDUAW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Alex's model\n",
        "\n",
        "cnn_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv3D(64, 5, activation='relu', data_format='channels_last', input_shape=(60,60,60,6), padding=\"same\"),\n",
        "        tf.keras.layers.MaxPool3D(data_format='channels_last'),\n",
        "        tf.keras.layers.Conv3D(128, 3, activation='relu', data_format='channels_last', padding=\"same\"),\n",
        "        tf.keras.layers.Conv3D(128, 3, activation='relu', data_format='channels_last', padding=\"same\"),\n",
        "        tf.keras.layers.MaxPool3D(data_format='channels_last'),\n",
        "        tf.keras.layers.Conv3D(256, 3, activation='relu', data_format='channels_last', padding=\"same\"),\n",
        "        tf.keras.layers.Conv3D(256, 3, activation='relu', data_format='channels_last', padding=\"same\"),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(units=128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(units=64, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(units=1),\n",
        "    ])\n",
        "\n",
        "cnn_model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=[\"RootMeanSquaredError\"])"
      ],
      "metadata": {
        "id": "RLkC3ZiaLkRm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "\n",
        "out_info = cnn_model.fit(train_dataset.batch(batch_size), epochs=20, validation_data=valid_dataset.batch(batch_size))\n",
        "mse_test = cnn_model.evaluate(test_dataset.batch(batch_size))\n",
        "#mse_test, rmse_test = cnn_model.evaluate(test.batch(BATCH))"
      ],
      "metadata": {
        "id": "czS-cMheL4TL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e66d07bb-a054-4003-cc6f-49423f1b1ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            " 234641/Unknown - 1592s 7ms/step"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Questions:\n",
        "\n",
        "1. What if we used a batch normalization in the model? Something like:\n",
        "\n",
        "```\n",
        "tf.keras.layers.BatchNormalization()\n",
        "```\n",
        "\n",
        "in the begining of the model and after each hidden layer?\n"
      ],
      "metadata": {
        "id": "psJtWtTITA6F"
      }
    }
  ]
}