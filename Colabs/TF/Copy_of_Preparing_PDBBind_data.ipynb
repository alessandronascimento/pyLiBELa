{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronascimento/pyLiBELa/blob/main/Colabs/TF/Copy_of_Preparing_PDBBind_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Conecting to Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/PDBbind_v2020/scripts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD_wZorcHhrB",
        "outputId": "313154e2-a264-4f36-b110-f42f82073ed9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/.shortcut-targets-by-id/1yaTdYVpjmL1DAJ6BUmgTH5HVaSbYH99b/PDBbind_v2020/scripts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Getting list of targets with grids\n",
        "targets_ok = []\n",
        "target_ok_file = open('targets_ok.dat', 'r')\n",
        "for line in target_ok_file:\n",
        "  line2 = line.split()\n",
        "  targets_ok.append(line2[0])\n",
        "target_ok_file.close()"
      ],
      "metadata": {
        "id": "Tut1ZZ_T48ed"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reading PDBBind v2020 data\n",
        "binding_targets = []             # pdb id\n",
        "binding_years = []               # year of the structure\n",
        "binding_resolution = []          # resolution\n",
        "binding_score = []               # -logKd/Ki\n",
        "\n",
        "binding_file = open('../index/INDEX_general_PL_data.2020', 'r')\n",
        "for line in binding_file:\n",
        "  line2 = line.split()\n",
        "  if line2[0][0] != '#':\n",
        "    if line2[0] not in targets_ok: continue\n",
        "\n",
        "    binding_targets.append(line2[0])\n",
        "    if (line2[1] == 'NMR'):\n",
        "      binding_resolution.append(0.00)\n",
        "    else:\n",
        "      binding_resolution.append(float(line2[1]))\n",
        "    binding_years.append(int(line2[2]))\n",
        "    binding_score.append(float(line2[3]))\n",
        "binding_file.close()\n",
        "\n",
        "print('Binding data found for %d valid targets' % len(binding_targets))"
      ],
      "metadata": {
        "id": "R_XETC5FIITe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caed2c86-df1f-40d0-8967-c35d5d099c4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binding data found for 18925 valid targets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Auxiliary functions\n",
        "\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "\n",
        "def process_file(file_path):\n",
        "\n",
        "  as_path = Path(file_path)\n",
        "  if not as_path.exists():\n",
        "    raise IOError(f\"File path '{as_path}' does not exist\")\n",
        "\n",
        "  data = tf.io.read_file(file_path)\n",
        "  data = tf.io.decode_raw(data, tf.float64)\n",
        "  data = tf.reshape(data, (60, 60, 60, 3))\n",
        "  return data\n",
        "\n",
        "def _log_file_not_found(file_path):\n",
        "  with open(\"log_files_not_found\", 'a') as f:\n",
        "    f.write(f'{file_path}\\n')\n",
        "\n",
        "\n",
        "def data_generator(name_list, score_list):\n",
        "  ntargets=20\n",
        "\n",
        "  for i in range(ntargets): #len(targets_ok)):\n",
        "\n",
        "    target = name_list[i].decode()\n",
        "    file_path = '../targets/{}/grid_30_0.5_SF0'.format(target)\n",
        "\n",
        "    try:\n",
        "      grid1 = process_file(file_path + '/McGrid_rec.grid')\n",
        "      grid2 = process_file(file_path + '/McGrid_lig.grid')\n",
        "\n",
        "    except IOError:\n",
        "      _log_file_not_found(file_path)\n",
        "      continue\n",
        "\n",
        "    observable = score_list[i]\n",
        "    combined_grid = tf.concat([grid1, grid2], axis=-1)\n",
        "\n",
        "    # Yield the data as a tuple\n",
        "    yield combined_grid, observable\n",
        "\n",
        "    # Yields the 10 related decoys of the current grid\n",
        "    for j in range(1,11):\n",
        "\n",
        "      grid2 = process_file(file_path + '/McGrid_dec_{}.grid'.format(j))\n",
        "\n",
        "      observable = 0.00\n",
        "      combined_grid = tf.concat([grid1, grid2], axis=-1)\n",
        "      yield combined_grid, observable\n"
      ],
      "metadata": {
        "id": "UOoG_Q3u5FH6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reading dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_names, test_names, train_scores, test_scores = train_test_split(binding_targets, binding_score, train_size=0.8, shuffle=True)\n",
        "train_names, valid_names, train_scores, valid_scores = train_test_split(train_names, train_scores, train_size=0.8)\n",
        "\n",
        "def slice_data(dataset):\n",
        "  x = tf.concat([x for x, y in dataset], axis=-1)\n",
        "  y = tf.concat([y for x, y in dataset], axis=-1)\n",
        "  return x,y\n",
        "\n",
        "# Some dataset parameters\n",
        "\n",
        "output_signature = (tf.TensorSpec(shape=(60, 60, 60, 6), dtype=tf.float64, name='combgrid'),\n",
        "                    tf.TensorSpec(shape=(), dtype=tf.float32, name='ligscore'))\n",
        "batch_size = 5\n",
        "prefetch_size = 1\n",
        "\n",
        "# Create the dataset from the generator function,\n",
        "# with batch and prefetch sizes already determined\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator,\n",
        "    output_signature=output_signature,\n",
        "    args=(tf.convert_to_tensor(train_names, dtype=tf.string), tf.convert_to_tensor(train_scores, dtype=tf.float32)),\n",
        "    name=\"train_dataset_gen\"\n",
        ").batch(batch_size).prefetch(prefetch_size)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator,\n",
        "    output_signature=output_signature,\n",
        "    args=(tf.convert_to_tensor(test_names, dtype=tf.string), tf.convert_to_tensor(test_scores, dtype=tf.float32)),\n",
        "    name=\"test_dataset_gen\"\n",
        ").batch(batch_size).prefetch(prefetch_size)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator,\n",
        "    output_signature=output_signature,\n",
        "    args=(tf.convert_to_tensor(valid_names, dtype=tf.string), tf.convert_to_tensor(valid_scores, dtype=tf.float32)),\n",
        "    name=\"valid_dataset_gen\"\n",
        ").batch(batch_size).prefetch(prefetch_size)\n",
        "\n",
        "# train_val, test = tf.keras.utils.split_dataset(dataset, left_size=0.8, right_size=0.2, shuffle=True, seed=23)\n",
        "# train, valid = tf.keras.utils.split_dataset(train_val, left_size=0.8, right_size=0.2)\n",
        "\n",
        "# x_train, y_train = slice_data(train)\n",
        "# x_test, y_test = slice_data(test)\n",
        "# x_valid, y_valid = slice_data(valid)\n",
        "\n",
        "# Repeat the dataset for multiple epochs (optional)\n",
        "#dataset = dataset.repeat()"
      ],
      "metadata": {
        "id": "_TE_iVYIDUAW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Alex's model\n",
        "\n",
        "cnn_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv3D(64, 5, activation='relu', data_format='channels_last', input_shape=(60,60,60,6), padding=\"same\"),\n",
        "        tf.keras.layers.MaxPool3D(data_format='channels_last'),\n",
        "        tf.keras.layers.Conv3D(128, 3, activation='relu', data_format='channels_last', padding=\"same\"),\n",
        "        tf.keras.layers.Conv3D(128, 3, activation='relu', data_format='channels_last', padding=\"same\"),\n",
        "        tf.keras.layers.MaxPool3D(data_format='channels_last'),\n",
        "        tf.keras.layers.Conv3D(256, 3, activation='relu', data_format='channels_last', padding=\"same\"),\n",
        "        tf.keras.layers.Conv3D(256, 3, activation='relu', data_format='channels_last', padding=\"same\"),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(units=128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(units=64, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(units=1),\n",
        "    ])\n",
        "\n",
        "cnn_model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=[\"RootMeanSquaredError\"])"
      ],
      "metadata": {
        "id": "RLkC3ZiaLkRm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_info = cnn_model.fit(train_dataset, epochs=20, validation_data=valid_dataset)\n",
        "mse_test = cnn_model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "czS-cMheL4TL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e28dc7-30bf-41f5-98fe-3ead67414cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "     11/Unknown - 618s 42s/step - loss: 112.2369 - root_mean_squared_error: 10.5942"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Questions:\n",
        "\n",
        "1. What if we used a batch normalization in the model? Something like:\n",
        "\n",
        "```\n",
        "tf.keras.layers.BatchNormalization()\n",
        "```\n",
        "\n",
        "in the begining of the model and after each hidden layer?\n"
      ],
      "metadata": {
        "id": "psJtWtTITA6F"
      }
    }
  ]
}