{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronascimento/pyLiBELa/blob/main/Colabs/TF/Copy_of_Preparing_PDBBind_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD_wZorcHhrB",
        "outputId": "8a2aadf0-7c42-4482-d8af-2b8176438ad8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/.shortcut-targets-by-id/1yaTdYVpjmL1DAJ6BUmgTH5HVaSbYH99b/PDBbind_v2020/scripts\n"
          ]
        }
      ],
      "source": [
        "#@title Conecting to Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/PDBbind_v2020/scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tut1ZZ_T48ed"
      },
      "outputs": [],
      "source": [
        "#@title Getting list of targets with grids\n",
        "targets_ok = []\n",
        "target_ok_file = open('targets_ok.dat', 'r')\n",
        "for line in target_ok_file:\n",
        "  line2 = line.split()\n",
        "  targets_ok.append(line2[0])\n",
        "target_ok_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_XETC5FIITe",
        "outputId": "021d6bd8-db07-45a1-bc6f-14b2b2c7fd68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binding data found for 5278 valid targets\n"
          ]
        }
      ],
      "source": [
        "#@title Reading PDBBind v2020 data\n",
        "binding_targets = []             # pdb id\n",
        "binding_years = []               # year of the structure\n",
        "binding_resolution = []          # resolution\n",
        "binding_score = []               # -logKd/Ki\n",
        "\n",
        "binding_file = open('../index/INDEX_refined_data.2020', 'r')\n",
        "for line in binding_file:\n",
        "  line2 = line.split()\n",
        "  if line2[0][0] != '#':\n",
        "    if line2[0] not in targets_ok: continue\n",
        "\n",
        "    binding_targets.append(line2[0])\n",
        "    if (line2[1] == 'NMR'):\n",
        "      binding_resolution.append(0.00)\n",
        "    else:\n",
        "      binding_resolution.append(float(line2[1]))\n",
        "    binding_years.append(int(line2[2]))\n",
        "    binding_score.append(float(line2[3]))\n",
        "binding_file.close()\n",
        "\n",
        "print('Binding data found for %d valid targets' % len(binding_targets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOoG_Q3u5FH6"
      },
      "outputs": [],
      "source": [
        "#@title Auxiliary functions\n",
        "\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "\n",
        "def process_file(file_path):\n",
        "\n",
        "  as_path = Path(file_path)\n",
        "  if not as_path.exists():\n",
        "    raise IOError(f\"File path '{as_path}' does not exist\")\n",
        "\n",
        "  data = tf.io.read_file(file_path)\n",
        "  data = tf.io.decode_raw(data, tf.float64)\n",
        "  data = tf.reshape(data, (60, 60, 60, 3))\n",
        "  return data\n",
        "\n",
        "def _log_file_not_found(file_path):\n",
        "  with open(\"log_files_not_found\", 'a') as f:\n",
        "    f.write(f'{file_path}\\n')\n",
        "\n",
        "\n",
        "def data_generator(name_list, score_list):\n",
        "  #ntargets=20\n",
        "\n",
        "  # Using the formula for class weights to stipulate the sample weights:\n",
        "  # W_class = n_total / (n_class * num_of_classes)\n",
        "  weight_normal = 11/2\n",
        "  weight_decoy  = 11/20\n",
        "\n",
        "  for i in range(len(binding_targets)):\n",
        "\n",
        "    target = name_list[i].decode()\n",
        "    file_path = '../targets/{}/grid_30_0.5_SF0'.format(target)\n",
        "\n",
        "    try:\n",
        "      grid1 = process_file(file_path + '/McGrid_rec.grid')\n",
        "      grid2 = process_file(file_path + '/McGrid_lig.grid')\n",
        "\n",
        "    except IOError:\n",
        "      _log_file_not_found(file_path)\n",
        "      continue\n",
        "\n",
        "    observable = score_list[i]\n",
        "    combined_grid = tf.concat([grid1, grid2], axis=-1)\n",
        "\n",
        "    # Yield the data as a tuple\n",
        "    yield combined_grid, observable, weight_normal\n",
        "\n",
        "    # Yields the 10 related decoys of the current grid\n",
        "    for j in range(1,11):\n",
        "\n",
        "      grid2 = process_file(file_path + '/McGrid_dec_{}.grid'.format(j))\n",
        "\n",
        "      observable = 0.00\n",
        "      combined_grid = tf.concat([grid1, grid2], axis=-1)\n",
        "      yield combined_grid, observable, weight_decoy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TE_iVYIDUAW"
      },
      "outputs": [],
      "source": [
        "#@title Reading dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_names, test_names, train_scores, test_scores = train_test_split(binding_targets, binding_score, train_size=0.8, shuffle=True)\n",
        "train_names, valid_names, train_scores, valid_scores = train_test_split(train_names, train_scores, train_size=0.8)\n",
        "\n",
        "def slice_data(dataset):\n",
        "  x = tf.concat([x for x, y in dataset], axis=-1)\n",
        "  y = tf.concat([y for x, y in dataset], axis=-1)\n",
        "  return x,y\n",
        "\n",
        "# Some dataset parameters\n",
        "\n",
        "output_signature = (tf.TensorSpec(shape=(60, 60, 60, 6), dtype=tf.float64, name='combGrid'),\n",
        "                    tf.TensorSpec(shape=(), dtype=tf.float32, name='ligScore'),\n",
        "                    tf.TensorSpec(shape=(), dtype=tf.float32, name='sampleWeight'))\n",
        "\n",
        "batch_size = 5\n",
        "prefetch_size = 1\n",
        "\n",
        "# Create the dataset from the generator function,\n",
        "# with batch and prefetch sizes already determined\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator,\n",
        "    output_signature=output_signature,\n",
        "    args=(tf.convert_to_tensor(train_names, dtype=tf.string), tf.convert_to_tensor(train_scores, dtype=tf.float32)),\n",
        "    name=\"train_dataset_gen\"\n",
        ").batch(batch_size).prefetch(prefetch_size)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator,\n",
        "    output_signature=output_signature,\n",
        "    args=(tf.convert_to_tensor(test_names, dtype=tf.string), tf.convert_to_tensor(test_scores, dtype=tf.float32)),\n",
        "    name=\"test_dataset_gen\"\n",
        ").batch(batch_size).prefetch(prefetch_size)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator,\n",
        "    output_signature=output_signature,\n",
        "    args=(tf.convert_to_tensor(valid_names, dtype=tf.string), tf.convert_to_tensor(valid_scores, dtype=tf.float32)),\n",
        "    name=\"valid_dataset_gen\"\n",
        ").batch(batch_size).prefetch(prefetch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLkC3ZiaLkRm"
      },
      "outputs": [],
      "source": [
        "#@title Alex's model\n",
        "\n",
        "cnn_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Conv3D(64, 5, activation='relu',\n",
        "                               data_format='channels_last', input_shape=(60,60,60,6), padding=\"same\"),\n",
        "        tf.keras.layers.MaxPool3D(data_format='channels_last'),\n",
        "        tf.keras.layers.Conv3D(128, 3, activation='relu',\n",
        "                               data_format='channels_last', padding=\"same\"),\n",
        "        tf.keras.layers.Conv3D(128, 3, activation='relu',\n",
        "                               data_format='channels_last', padding=\"same\"),\n",
        "        tf.keras.layers.MaxPool3D(data_format='channels_last'),\n",
        "        tf.keras.layers.Conv3D(256, 3, activation='relu',\n",
        "                               data_format='channels_last', padding=\"same\"),\n",
        "        tf.keras.layers.Conv3D(256, 3, activation='relu',\n",
        "                               data_format='channels_last', padding=\"same\"),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(units=128, activation=\"relu\"),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(units=64, activation=\"relu\"),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(units=1),\n",
        "    ])\n",
        "\n",
        "cnn_model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=[\"RootMeanSquaredError\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "czS-cMheL4TL",
        "outputId": "54f61ecd-b8d1-4d0c-9ce3-85f16274afff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    427/Unknown - 17967s 42s/step - loss: 13.2974 - root_mean_squared_error: 2.5304"
          ]
        }
      ],
      "source": [
        "out_info = cnn_model.fit(train_dataset, epochs=1, validation_data=valid_dataset)\n",
        "mse_test = cnn_model.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdF9AdR_NFGz"
      },
      "source": [
        "Notes to self:\n",
        "  Don't know the effect of serving a sample_weight term for the validation and test dataset generator"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}